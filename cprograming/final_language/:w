#include <stdio.h>
#include <string.h>
#include <stdlib.h>

typedef enum {
	LPAREN, RPAREN, LBRACE, RBRACE, LSQUARE, RSQUARE, LANGLED, RANGLED, //bracket types	
	PLUS, MINUS, TIMES, DIVIDE, MOD, AND, OR, POWER, NOT, EQUAL,//arithmetic
	FUNCTION, LET, IF, FOR, WHILE, RETURN, //keywords
	NUMBER, IDENTIFIER, STRING, CHAR //vaiables 
} token_type;  

typedef struct token {
	token_type type;
	char * data_string;
	void * token_data;
} token;

typedef struct tokenizer {
	FILE * target_file;
	char * current_token_string;
	int current_token_string_size;
	token ** tokens; 
	int token_count; 
	int max_tokens;
} tokenizer;

token * token_create(token_type type, char * token_string){
	token * new_token = calloc(1, sizeof(token));
	new_token -> type = type;
	new_token -> data_string = strdup(token_string);
	new_token -> token_data = NULL;
	return new_token;
}

void token_free(token * token) {
	free(token->data_string);
	free(token);
}

tokenizer * tokenizer_create(char * file_name, int max_tokens){
	tokenizer * new_tokenizer = calloc(1, sizeof(tokenizer));
	new_tokenizer->target_file = fopen(file_name, "r");
	if (new_tokenizer->target_file == NULL || max_tokens == 0){
		printf("File %s could not be read or invalid max tokens. Exiting.\n",file_name);
		free(new_tokenizer);
		exit(0);
	}
	new_tokenizer->current_token_string_size = 256; 
	new_tokenizer->current_token_string = calloc(new_tokenizer->current_token_string_size, sizeof(char));
	new_tokenizer->token_count = 0;
	new_tokenizer->max_tokens = max_tokens;
	new_tokenizer->tokens= calloc(new_tokenizer->max_tokens, sizeof(token *));
	return new_tokenizer;
}

void tokenizer_free(tokenizer * tokenizer){
	fclose(tokenizer->target_file);
	free(tokenizer->current_token_string);
	for (int i = 0; i< tokenizer->token_count; i++){
		token_free(tokenizer->tokens[i]);
		tokenizer->tokens[i] = NULL;	
	}
	free(tokenizer->tokens);
}

void tokenizer_test_validity(tokenizer * tokenizer){
	if (tokenizer->max_tokens < tokenizer->token_count) {
		printf("Invalid tokenizer. Max tokens less than token count. Exiting.\n");	
		exit(0);
	}

	for (int i = 0; i < tokenizer->token_count; i++) {
		for (int j = 0; j < tokenizer->token_count; j++) {
			if (i != j && tokenizer->tokens[i] == tokenizer->tokens[j]) {
				printf("Invalid tokenizer. Repeated token. Exiting.\n");	
				exit(0);
			}
		}	
	}	

	for (int i = tokenizer->token_count; i < tokenizer->max_tokens; i++){
		if (tokenizer->tokens[i] != NULL) {
			printf("Invalid tokenizer. Non-null tokens past reported token count. Exiting.\n");	
			exit(0);
		}
	}	

	if (tokenizer->current_token_string == NULL){
		printf("Invalid tokenizer. Current token string is NULL. Exiting.\n");	
		exit(0);
	}
}

void tokenizer_check_and_resize(tokenizer * tokenizer){
	if(tokenizer->max_tokens== tokenizer->token_count){
		tokenizer->max_tokens*= 2;
		tokenizer->tokens = realloc(tokenizer->tokens, tokenizer->max_tokens*sizeof(token *));
	}		
}

void token_print(token * print_token, FILE * print_destination){
	if (print_destination == NULL) {
		print_destination = stdout;
	}
	fprintf(print_destination,"Type: %d, string: %s.\n", print_token->type, print_token->data_string);
};

void tokenizer_print(tokenizer * print_tokenizer, FILE * print_destination){
	if (print_destination == NULL) {
		print_destination = stdout;
	}

	fprintf(print_destination,"Tokenizer stats: max tokens %d, current tokens %d.\n\n",print_tokenizer->max_tokens, print_tokenizer->token_count);

	for (int i = 0; i< print_tokenizer->token_count; i++){
		token_print(print_tokenizer->tokens[i], print_destination);
	}
};

void tokenizer_add_token(tokenizer * tokenizer, char * token_string, token_type type){
	tokenizer_check_and_resize(tokenizer);	
	if (token_string != NULL) {
		tokenizer->tokens[tokenizer->token_count] = token_create(type, token_string); 
		tokenizer->token_count += 1;
	} else if (tokenizer->current_token_string != NULL){
		tokenizer->tokens[tokenizer->token_count] = token_create(type, tokenizer->current_token_string); 
		tokenizer->token_count += 1;
	}
}

void tokenizer_get_next_token(tokenizer * tokenizer){
	int current_char = fgetc(tokenizer->target_file);
	switch (current_char) {
		case ' ':
		case '\n':
		case '\t':
			tokenizer_get_next_token(tokenizer); //in case of whitespace, just skip it...
			break;
		case '(': tokenizer_add_token(tokenizer, "(", LPAREN);break;
		case ')': tokenizer_add_token(tokenizer, ")", LPAREN);break;
		case '{': tokenizer_add_token(tokenizer, "{", LPAREN);break;
		case '}': tokenizer_add_token(tokenizer, "}", LPAREN);break;
		case '[': tokenizer_add_token(tokenizer, "[", LPAREN);break;
		case ']': tokenizer_add_token(tokenizer, "]", LPAREN);break;
		case '<': tokenizer_add_token(tokenizer, "<", LPAREN);break;
		case '>': tokenizer_add_token(tokenizer, ">", LPAREN);break;
		case '+': tokenizer_add_token(tokenizer, "+", LPAREN);break;
		case '-': tokenizer_add_token(tokenizer, "-", LPAREN);break;
		case '*': tokenizer_add_token(tokenizer, "*", LPAREN);break;
		case '/': tokenizer_add_token(tokenizer, "(", LPAREN);break;
		case '%': tokenizer_add_token(tokenizer, "(", LPAREN);break;
		case '&': tokenizer_add_token(tokenizer, "(", LPAREN);break;
		case '|': tokenizer_add_token(tokenizer, "(", LPAREN);break;
		case '^': tokenizer_add_token(tokenizer, "(", LPAREN);break;
		case '!': tokenizer_add_token(tokenizer, "(", LPAREN);break;
		case '=': tokenizer_add_token(tokenizer, "(", LPAREN);break;
		default:
			printf("Must be looking at a keyword or a variable/value\n");	
			break;
			
	}
}
	LPAREN, RPAREN, LBRACE, RBRACE, LSQUARE, RSQUARE, LANGLED, RANGLED, //bracket types	
	PLUS, MINUS, TIMES, DIVIDE, MOD, AND, OR, POWER, NOT, EQUAL,//arithmetic
	FUNCTION, LET, IF, FOR, WHILE, RETURN, //keywords
	NUMBER, IDENTIFIER, STRING, CHAR //vaiables 

void tokenizer_tokenize_file() {

}

